{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"cyte-seek \ud83d\udd2c An Awesome Single-cell CITE-sequencing Pipeline This is the home of the pipeline, cyte-seek. Its long-term goals: to accurately perform cell filtering, normalization, clustering, differential expression analysis, and cell type prediction like no pipeline before! Overview \u00b6 Welcome to cyte-seek's documentation! This guide is the main source of documentation for users that are getting started with the single-cell CITE-sequencing pipeline . Before getting started, we highly recommend reading through the usage section of each available sub command. The ./cyte-seek pipeline is composed several inter-related sub commands to setup and run the pipeline across different systems. Each of the available sub commands perform different functions: cyte-seek run : Run the cyte-seek pipeline with your input files. cyte-seek unlock : Unlocks a previous runs output directory. cyte-seek cache : Cache software containers locally. cyte-seek is a comprehensive single-cell pipeline optimized for CITE-sequencing data. It relies on technologies like Singularity 1 to maintain the highest-level of reproducibility. The pipeline consists of a series of data processing and quality-control steps orchestrated by Snakemake 2 , a flexible and scalable workflow management system, to submit jobs to a cluster. The pipeline is compatible with data generated from 10x sequencing technologies. The CITE-seq analysis pipeline starts from sample FASTQ files and performs initial processing of the data. Starting from Cell Ranger analysis, each sample undergoes cell filtering, data normalization, clustering, differential expression analysis, and cell type prediction. The processed samples are also integrated together to perform clustering, differential expression, and cell type prediction on a project level. Additionally, if genotype information is provided then genetic multiplexing of each sample is performed. The pipeline can be run locally on a compute instance or on-premise using a cluster. A user can define the method or mode of execution. For more information about issues or trouble-shooting a problem, please checkout our FAQ prior to opening an issue on Github . If you have any questions, please feel free to start a discussion . Contribute \u00b6 This site is a living document, created for and by members like you. cyte-seek is maintained by the members of OpenOmics and is improved by continous feedback! We encourage you to contribute new content and make improvements to existing content via pull request to our GitHub repository . References \u00b6 1. Kurtzer GM, Sochat V, Bauer MW (2017). Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459. 2. Koster, J. and S. Rahmann (2018). \"Snakemake-a scalable bioinformatics workflow engine.\" Bioinformatics 34(20): 3600.","title":"About"},{"location":"#overview","text":"Welcome to cyte-seek's documentation! This guide is the main source of documentation for users that are getting started with the single-cell CITE-sequencing pipeline . Before getting started, we highly recommend reading through the usage section of each available sub command. The ./cyte-seek pipeline is composed several inter-related sub commands to setup and run the pipeline across different systems. Each of the available sub commands perform different functions: cyte-seek run : Run the cyte-seek pipeline with your input files. cyte-seek unlock : Unlocks a previous runs output directory. cyte-seek cache : Cache software containers locally. cyte-seek is a comprehensive single-cell pipeline optimized for CITE-sequencing data. It relies on technologies like Singularity 1 to maintain the highest-level of reproducibility. The pipeline consists of a series of data processing and quality-control steps orchestrated by Snakemake 2 , a flexible and scalable workflow management system, to submit jobs to a cluster. The pipeline is compatible with data generated from 10x sequencing technologies. The CITE-seq analysis pipeline starts from sample FASTQ files and performs initial processing of the data. Starting from Cell Ranger analysis, each sample undergoes cell filtering, data normalization, clustering, differential expression analysis, and cell type prediction. The processed samples are also integrated together to perform clustering, differential expression, and cell type prediction on a project level. Additionally, if genotype information is provided then genetic multiplexing of each sample is performed. The pipeline can be run locally on a compute instance or on-premise using a cluster. A user can define the method or mode of execution. For more information about issues or trouble-shooting a problem, please checkout our FAQ prior to opening an issue on Github . If you have any questions, please feel free to start a discussion .","title":"Overview"},{"location":"#contribute","text":"This site is a living document, created for and by members like you. cyte-seek is maintained by the members of OpenOmics and is improved by continous feedback! We encourage you to contribute new content and make improvements to existing content via pull request to our GitHub repository .","title":"Contribute"},{"location":"#references","text":"1. Kurtzer GM, Sochat V, Bauer MW (2017). Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459. 2. Koster, J. and S. Rahmann (2018). \"Snakemake-a scalable bioinformatics workflow engine.\" Bioinformatics 34(20): 3600.","title":"References"},{"location":"license/","text":"MIT License \u00b6 Copyright \u00a9 2022 OpenOmics Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#mit-license","text":"Copyright \u00a9 2022 OpenOmics Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"faq/questions/","text":"Frequently Asked Questions \u00b6 This page is still under construction. If you need immediate help, please open an issue on Github!","title":"General Questions"},{"location":"faq/questions/#frequently-asked-questions","text":"This page is still under construction. If you need immediate help, please open an issue on Github!","title":"Frequently Asked Questions"},{"location":"usage/cache/","text":"cyte-seek cache \u00b6 1. About \u00b6 The cyte-seek executable is composed of several inter-related sub commands. Please see cyte-seek -h for all available options. This part of the documentation describes options and concepts for cyte-seek cache sub command in more detail. With minimal configuration, the cache sub command enables you to cache remote software containers from Dockerhub . Caching remote software containers allows the pipeline to run in an offline mode where no requests are made. The cache sub command can also be used to pull our pre-built software container onto a new cluster or target system. These containers are normally pulled onto the filesystem when the pipeline runs; however, due to network issues or DockerHub pull rate limits, it may make sense to pull the resources once so a shared cache can be created. It is worth noting that a singularity cache cannot normally be shared across users. Singularity strictly enforces that a cache is owned by the user. To get around this issue, the cache subcommand can be used to create local SIFs on the filesystem from images on DockerHub. The path of these locally cached SIFs can be passed to the run sub commands --sif-cache option. Caching software containers is fast and easy! In its most basic form, cyte-seek cache only has one required input . 2. Synopsis \u00b6 $ ./cyte-seek cache [--help] [--dry-run] \\ --sif-cache SIF_CACHE The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide a directory to cache remote Docker images via the --sif-cache argument. Once the cache has pipeline completed, the local sif cache can be passed to the --sif-cache option of the cyte-seek run subcomand. This enables the pipeline to run in an offline mode. Use you can always use the -h option for information on a specific command. 2.1 Required Arguments \u00b6 --sif-cache SIF_CACHE Path where a local cache of SIFs will be stored. type: path Any images defined in config/containers.json will be pulled into the local filesystem. The path provided to this option can be passed to the --sif-cache option of the cyte-seek run subcomand. This allows for running the build and run pipelines in an offline mode where no requests are made to external sources. This is useful for avoiding network issues or DockerHub pull rate limits. Please see cyte-seek run for more information. Example: --sif-cache /data/$USER/cache 2.2 Options \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help --dry-run Dry run the pipeline. type: boolean flag Only displays what software container will be cached locally. Does not execute anything! Example: --dry-run 3. Example \u00b6 # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load singularity snakemake # Step 1.) Dry run to see what will be pulled ./cyte-seek cache --sif-cache /data/ $USER /cache \\ --dry-run # Step 2.) Cache remote resources locally. # This command will NOT automatically submit # a job to the cluster. As so, we recommend # submitting this next command to the cluster # as a job. Download speeds will vary so it # is best to set the wall time a few hours. ./cyte-seek cache --sif-cache /data/ $USER /cache","title":"cyte-seek cache"},{"location":"usage/cache/#cyte-seek-cache","text":"","title":"cyte-seek cache"},{"location":"usage/cache/#1-about","text":"The cyte-seek executable is composed of several inter-related sub commands. Please see cyte-seek -h for all available options. This part of the documentation describes options and concepts for cyte-seek cache sub command in more detail. With minimal configuration, the cache sub command enables you to cache remote software containers from Dockerhub . Caching remote software containers allows the pipeline to run in an offline mode where no requests are made. The cache sub command can also be used to pull our pre-built software container onto a new cluster or target system. These containers are normally pulled onto the filesystem when the pipeline runs; however, due to network issues or DockerHub pull rate limits, it may make sense to pull the resources once so a shared cache can be created. It is worth noting that a singularity cache cannot normally be shared across users. Singularity strictly enforces that a cache is owned by the user. To get around this issue, the cache subcommand can be used to create local SIFs on the filesystem from images on DockerHub. The path of these locally cached SIFs can be passed to the run sub commands --sif-cache option. Caching software containers is fast and easy! In its most basic form, cyte-seek cache only has one required input .","title":"1. About"},{"location":"usage/cache/#2-synopsis","text":"$ ./cyte-seek cache [--help] [--dry-run] \\ --sif-cache SIF_CACHE The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide a directory to cache remote Docker images via the --sif-cache argument. Once the cache has pipeline completed, the local sif cache can be passed to the --sif-cache option of the cyte-seek run subcomand. This enables the pipeline to run in an offline mode. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"usage/cache/#21-required-arguments","text":"--sif-cache SIF_CACHE Path where a local cache of SIFs will be stored. type: path Any images defined in config/containers.json will be pulled into the local filesystem. The path provided to this option can be passed to the --sif-cache option of the cyte-seek run subcomand. This allows for running the build and run pipelines in an offline mode where no requests are made to external sources. This is useful for avoiding network issues or DockerHub pull rate limits. Please see cyte-seek run for more information. Example: --sif-cache /data/$USER/cache","title":"2.1 Required Arguments"},{"location":"usage/cache/#22-options","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help --dry-run Dry run the pipeline. type: boolean flag Only displays what software container will be cached locally. Does not execute anything! Example: --dry-run","title":"2.2 Options"},{"location":"usage/cache/#3-example","text":"# Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load singularity snakemake # Step 1.) Dry run to see what will be pulled ./cyte-seek cache --sif-cache /data/ $USER /cache \\ --dry-run # Step 2.) Cache remote resources locally. # This command will NOT automatically submit # a job to the cluster. As so, we recommend # submitting this next command to the cluster # as a job. Download speeds will vary so it # is best to set the wall time a few hours. ./cyte-seek cache --sif-cache /data/ $USER /cache","title":"3. Example"},{"location":"usage/run/","text":"cyte-seek run \u00b6 1. About \u00b6 The cyte-seek executable is composed of several inter-related sub commands. Please see cyte-seek -h for all available options. This part of the documentation describes options and concepts for cyte-seek run sub command in more detail. With minimal configuration, the run sub command enables you to start running cyte-seek pipeline. Setting up the cyte-seek pipeline is fast and easy! In its most basic form, cyte-seek run only has five required inputs . 2. Synopsis \u00b6 $ cyte-seek run [--help] [--mode {slurm,uge,local}] [--job-name JOB_NAME] \\ [ --dry-run] [--silent] [--sif-cache SIF_CACHE] \\ [--singularity-cache SINGULARITY_CACHE] \\ [--tmp-dir TMP_DIR] [--threads THREADS] \\ [--pre-rna] [--force-cells] \\ [--num-cells NUM_CELLS] \\ --input INPUT [INPUT ...] \\ --output OUTPUT \\ --genome {hg38, ...} \\ --libraries LIBRARIES \\ --features FEATURES The synopsis for each command shows its arguments and their usage. Optional arguments are shown in square brackets. A user must provide a list of FastQ (globbing is supported) to analyze via --input argument, an output directory to store results via --output argument, a reference genome for alignment and cell-type prediction via the --genome arguement, a libraries file via the --libraries argument, and a features barcode file via a --features argument. Use you can always use the -h option for information on a specific command. 2.1 Required arguments \u00b6 Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. --input INPUT [INPUT ...] Input FastQ file(s). type: file(s) One or more FastQ files can be provided. The pipeline does NOT support single-end data. From the command-line, each input file should seperated by a space. Globbing is supported! This makes selecting FastQ files easy. Input FastQ files should always be gzipp-ed. Example: --input .tests/*.R?.fastq.gz --output OUTPUT Path to an output directory. type: path This location is where the pipeline will create all of its output files, also known as the pipeline's working directory. If the provided output directory does not exist, it will be created automatically. Example: --output /data/$USER/output --genome {hg38, ...} Reference genome. type: string This option defines the reference genome of the samples. cyte-seek does comes bundled with prebuilt reference files for human and mouse samples, e.g. hg38. Please select one of the following options: hg38. Please note that the mouse reference genome, mm10, is coming soon! Example: --genome hg38 --libraries LIBRARIES Libraries file. type: file A CSV file containing information about each library. It contains each sample's name, flowcell, demultiplexed name, and library type. More information about the libraries file and its requirements can be found on the 10x Genomics website . Here is an example libraries.csv file: Name,Flowcell,Sample,Type IL15_LNs,H7CNNBGXG,IL15_LNs,Gene Expression IL15_LNs,H7CT7BGXG,IL15_LNs_BC,Antibody Capture Where: Name: name of the sample passed to CellRanger. Flowcell: The flowcell ID that contains the FASTQ files for this set of data. Sample: Name that was used when demultiplexing, this should match the FASTQ files. Type: library type for each sample. List of supported options: Gene Expression CRISPR Guide Capture Antibody Capture Custom Example: --libraries libraries.csv --features FEATURES Features file. type: file A feature reference CSV file containing information for processing a feature barcode data. This file should contain a unique ID for the feature, a human readable name, sequence, feature type, read, and pattern. More information about the libraries file and its requirements can be found on the 10x Genomics website . Here is an example features.csv file: id,name,sequence,feature_type,read,pattern CITE_CD64,CD64,AGTGGG,Antibody Capture,R2,5PNN(BC) CITE_CD8,CD8,TCACCGT,Antibody Capture,R2,5PNNN(BC) Where: id: Unique ID for this feature. Must not contain whitespace, quote or comma characters. Each ID must be unique and must not collide with a gene identifier from the transcriptome. name: Human-readable name for this feature. Must not contain whitespace. sequence: Nucleotide barcode sequence associated with this feature, e.g. the antibody barcode or sgRNA protospacer sequence. read: Specifies which RNA sequencing read contains the Feature Barcode sequence. Must be R1 or R2, but in most cases R2 is the correct read. pattern: Specifies how to extract the sequence of the feature barcode from the read. Type: Type of the feature. List of supported options: Gene Expression CRISPR Guide Capture Antibody Capture Custom Example: --features features.csv 2.2 Analysis options \u00b6 Each of the following arguments are optional, and do not need to be provided. --num-cells NUM_CELLS Expected number of recovered cells. type: int default: 3000 Overrides the expected number of cells passed to cellranger. Example: --num-cells 4000 --force-cells Use expected cells counts. type: boolean Force pipeline to use the expected number of cells, which will bypass the cell detection algorithm. Example: --force-cells --pre-mrna Consider pre-mRNA. type: boolean Retain intronic sequences for consideration of pre-mRNA. Example: --pre-mrna --demuxlet Perform demuxlet analysis. type: boolean Perform demuxlet analysis for this project. This option requires a vcf file is provided. Please see the option below for more information. Example: --demuxlet --vcf VCF VCF file used for demuxlet analysis. type: file This option should be used with the demuxlet option above. Example: --vcf analysis.vcf --patient-list PATIENT_LIST Define patients associated with each sample for demuxlet analysis. type: file A CSV file used to define the patients associated with each single cell sample used in demuxlet analysis. Here is an example features.csv file: Sample1,Sample_2 patient1,patient2 patient2,patient4 patient3, Where: first row : contains the single cell sample names matching the Cell Ranger output following row(s): contain the patients associated with each single cell sample, using the same patient ID listing as listed in the vcf file. Example: --patient-list demuxlet.csv 2.4 Orchestration options \u00b6 Each of the following arguments are optional, and do not need to be provided. --dry-run Dry run the pipeline. type: boolean flag Displays what steps in the pipeline remain or will be run. Does not execute anything! Example: --dry-run --silent Silence standard output. type: boolean flag Reduces the amount of information directed to standard output when submitting master job to the job scheduler. Only the job id of the master job is returned. Example: --silent --mode {slurm,uge,local} Execution Method. type: string default: slurm Execution Method. Defines the mode or method of execution. Vaild mode options include: slurm, uge, or local. It is recommended to run cyte-seek on a cluster to reduce run times. At the current moment, the pipeline supports the following job schedulers: SLURM, UGE. slurm The slurm execution backend will submit jobs to the SLURM workload manager . This method will submit jobs to a cluster using sbatch. Please set the mode to slurm when running the pipeline on Biowulf. uge The uge execution backend will submit jobs to the UGE workload manager . This method will submit jobs to a cluster using qsub. Please set the mode to uge when running the pipeline on LOCUS. local Local executions will run serially on compute instance. This is useful for testing, debugging, or when a users does not have access to a high performance computing environment. If this option is not provided, it will default to a local execution mode. We do not recommend using this option. Example: --mode uge --job-name JOB_NAME Set the name of the pipeline's master job. type: string default: cyte-seek When submitting the pipeline to a job scheduler, like SLURM, this option always you to set the name of the pipeline's master job. By default, the name of the pipeline's master job is set to \"cyte-seek\". Example: --job-name pl_id-42 --singularity-cache SINGULARITY_CACHE Overrides the $SINGULARITY_CACHEDIR environment variable. type: path default: --output OUTPUT/.singularity Singularity will cache image layers pulled from remote registries. This ultimately speeds up the process of pull an image from DockerHub if an image layer already exists in the singularity cache directory. By default, the cache is set to the value provided to the --output argument. Please note that this cache cannot be shared across users. Singularity strictly enforces you own the cache directory and will return a non-zero exit code if you do not own the cache directory! See the --sif-cache option to create a shareable resource. Example: --singularity-cache /data/$USER/.singularity --sif-cache SIF_CACHE Path where a local cache of SIFs are stored. type: path Uses a local cache of SIFs on the filesystem. This SIF cache can be shared across users if permissions are set correctly. If a SIF does not exist in the SIF cache, the image will be pulled from Dockerhub and a warning message will be displayed. The cyte-seek cache subcommand can be used to create a local SIF cache. Please see cyte-seek cache for more information. This command is extremely useful for avoiding DockerHub pull rate limits. It also remove any potential errors that could occur due to network issues or DockerHub being temporarily unavailable. We recommend running cyte-seek with this option when ever possible. Example: --singularity-cache /data/$USER/SIFs --threads THREADS Max number of threads for local processes. type: int default: 2 Max number of threads for local processes. This option is more applicable when running the pipeline with --mode local . It is recommended setting this vaule to the maximum number of CPUs available on the host machine. Example: --threads 12 --tmp-dir TMP_DIR Max number of threads for each process. type: path default: /tmp/$USER Path on the file system for writing temporary output files. By default, the temporary directory is set to '/lscratch/$SLURM_JOBID' for backwards compatibility with the NIH's Biowulf cluster; however, if you are running the pipeline on another cluster, this option will need to be specified. Ideally, this path should point to a dedicated location on the filesystem for writing tmp files. On many systems, this location is set to somewhere in /scratch. If you need to inject a variable into this string that should NOT be expanded, please quote this options value in single quotes. Example: --tmp-dir /hpcdata/scratch/$USER 2.5 Miscellaneous options \u00b6 Each of the following arguments are optional, and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help 3. Example \u00b6 3.1 Biowulf \u00b6 # Step 1.) Grab an interactive node, # do not run on head node! srun -N 1 -n 1 --time = 1 :00:00 --mem = 8gb --cpus-per-task = 2 --pty bash module purge module load singularity snakemake # Step 2A.) Dry-run the pipeline ./cyte-seek run --input .tests/*.R?.fastq.gz \\ --output /data/ $USER /cyte-seek_output \\ --features .tests/features.csv \\ --libraries libraries.csv \\ --genome hg38 \\ --mode slurm \\ --tmp-dir '/lscratch/$SLURM_JOBID' \\ --dry-run # Step 2B.) Run the cyte-seek pipeline # The slurm mode will submit jobs to # the cluster. It is recommended running # the pipeline in this mode on Biowulf. ./cyte-seek run --input .tests/*.R?.fastq.gz \\ --output /data/ $USER /cyte-seek_output \\ --features .tests/features.csv \\ --libraries libraries.csv \\ --genome hg38 \\ --mode slurm \\ --tmp-dir '/lscratch/$SLURM_JOBID' 3.2 LOCUS \u00b6 # Step 1.) Grab an interactive node, # do not run on head node! qrsh -l h_vmem = 4G -pe threaded 4 module load snakemake/6.0.5-Python-3.9.2 # Step 2A.) Dry-run the pipeline /hpcdata/rtb/NCBR/cyte-seek/v1.1.1/cyte-seek run \\ --input .tests/*.R?.fastq.gz \\ --output /hpcdata/scratch/ $USER /cyte-seek_output \\ --features .tests/features.csv \\ --libraries libraries.csv \\ --genome hg38 \\ --mode uge \\ --dry-run # Step 2B.) Run the cyte-seek pipeline # The slurm mode will submit jobs to # the cluster. It is recommended running # the pipeline in this mode on Biowulf. /hpcdata/rtb/NCBR/cyte-seek/v1.1.1/cyte-seek run \\ --input .tests/*.R?.fastq.gz \\ --output /hpcdata/scratch/ $USER /cyte-seek_output \\ --features .tests/features.csv \\ --libraries libraries.csv \\ --genome hg38 \\ --mode uge","title":"cyte-seek run"},{"location":"usage/run/#cyte-seek-run","text":"","title":"cyte-seek run"},{"location":"usage/run/#1-about","text":"The cyte-seek executable is composed of several inter-related sub commands. Please see cyte-seek -h for all available options. This part of the documentation describes options and concepts for cyte-seek run sub command in more detail. With minimal configuration, the run sub command enables you to start running cyte-seek pipeline. Setting up the cyte-seek pipeline is fast and easy! In its most basic form, cyte-seek run only has five required inputs .","title":"1. About"},{"location":"usage/run/#2-synopsis","text":"$ cyte-seek run [--help] [--mode {slurm,uge,local}] [--job-name JOB_NAME] \\ [ --dry-run] [--silent] [--sif-cache SIF_CACHE] \\ [--singularity-cache SINGULARITY_CACHE] \\ [--tmp-dir TMP_DIR] [--threads THREADS] \\ [--pre-rna] [--force-cells] \\ [--num-cells NUM_CELLS] \\ --input INPUT [INPUT ...] \\ --output OUTPUT \\ --genome {hg38, ...} \\ --libraries LIBRARIES \\ --features FEATURES The synopsis for each command shows its arguments and their usage. Optional arguments are shown in square brackets. A user must provide a list of FastQ (globbing is supported) to analyze via --input argument, an output directory to store results via --output argument, a reference genome for alignment and cell-type prediction via the --genome arguement, a libraries file via the --libraries argument, and a features barcode file via a --features argument. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"usage/run/#21-required-arguments","text":"Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. --input INPUT [INPUT ...] Input FastQ file(s). type: file(s) One or more FastQ files can be provided. The pipeline does NOT support single-end data. From the command-line, each input file should seperated by a space. Globbing is supported! This makes selecting FastQ files easy. Input FastQ files should always be gzipp-ed. Example: --input .tests/*.R?.fastq.gz --output OUTPUT Path to an output directory. type: path This location is where the pipeline will create all of its output files, also known as the pipeline's working directory. If the provided output directory does not exist, it will be created automatically. Example: --output /data/$USER/output --genome {hg38, ...} Reference genome. type: string This option defines the reference genome of the samples. cyte-seek does comes bundled with prebuilt reference files for human and mouse samples, e.g. hg38. Please select one of the following options: hg38. Please note that the mouse reference genome, mm10, is coming soon! Example: --genome hg38 --libraries LIBRARIES Libraries file. type: file A CSV file containing information about each library. It contains each sample's name, flowcell, demultiplexed name, and library type. More information about the libraries file and its requirements can be found on the 10x Genomics website . Here is an example libraries.csv file: Name,Flowcell,Sample,Type IL15_LNs,H7CNNBGXG,IL15_LNs,Gene Expression IL15_LNs,H7CT7BGXG,IL15_LNs_BC,Antibody Capture Where: Name: name of the sample passed to CellRanger. Flowcell: The flowcell ID that contains the FASTQ files for this set of data. Sample: Name that was used when demultiplexing, this should match the FASTQ files. Type: library type for each sample. List of supported options: Gene Expression CRISPR Guide Capture Antibody Capture Custom Example: --libraries libraries.csv --features FEATURES Features file. type: file A feature reference CSV file containing information for processing a feature barcode data. This file should contain a unique ID for the feature, a human readable name, sequence, feature type, read, and pattern. More information about the libraries file and its requirements can be found on the 10x Genomics website . Here is an example features.csv file: id,name,sequence,feature_type,read,pattern CITE_CD64,CD64,AGTGGG,Antibody Capture,R2,5PNN(BC) CITE_CD8,CD8,TCACCGT,Antibody Capture,R2,5PNNN(BC) Where: id: Unique ID for this feature. Must not contain whitespace, quote or comma characters. Each ID must be unique and must not collide with a gene identifier from the transcriptome. name: Human-readable name for this feature. Must not contain whitespace. sequence: Nucleotide barcode sequence associated with this feature, e.g. the antibody barcode or sgRNA protospacer sequence. read: Specifies which RNA sequencing read contains the Feature Barcode sequence. Must be R1 or R2, but in most cases R2 is the correct read. pattern: Specifies how to extract the sequence of the feature barcode from the read. Type: Type of the feature. List of supported options: Gene Expression CRISPR Guide Capture Antibody Capture Custom Example: --features features.csv","title":"2.1 Required arguments"},{"location":"usage/run/#22-analysis-options","text":"Each of the following arguments are optional, and do not need to be provided. --num-cells NUM_CELLS Expected number of recovered cells. type: int default: 3000 Overrides the expected number of cells passed to cellranger. Example: --num-cells 4000 --force-cells Use expected cells counts. type: boolean Force pipeline to use the expected number of cells, which will bypass the cell detection algorithm. Example: --force-cells --pre-mrna Consider pre-mRNA. type: boolean Retain intronic sequences for consideration of pre-mRNA. Example: --pre-mrna --demuxlet Perform demuxlet analysis. type: boolean Perform demuxlet analysis for this project. This option requires a vcf file is provided. Please see the option below for more information. Example: --demuxlet --vcf VCF VCF file used for demuxlet analysis. type: file This option should be used with the demuxlet option above. Example: --vcf analysis.vcf --patient-list PATIENT_LIST Define patients associated with each sample for demuxlet analysis. type: file A CSV file used to define the patients associated with each single cell sample used in demuxlet analysis. Here is an example features.csv file: Sample1,Sample_2 patient1,patient2 patient2,patient4 patient3, Where: first row : contains the single cell sample names matching the Cell Ranger output following row(s): contain the patients associated with each single cell sample, using the same patient ID listing as listed in the vcf file. Example: --patient-list demuxlet.csv","title":"2.2 Analysis options"},{"location":"usage/run/#24-orchestration-options","text":"Each of the following arguments are optional, and do not need to be provided. --dry-run Dry run the pipeline. type: boolean flag Displays what steps in the pipeline remain or will be run. Does not execute anything! Example: --dry-run --silent Silence standard output. type: boolean flag Reduces the amount of information directed to standard output when submitting master job to the job scheduler. Only the job id of the master job is returned. Example: --silent --mode {slurm,uge,local} Execution Method. type: string default: slurm Execution Method. Defines the mode or method of execution. Vaild mode options include: slurm, uge, or local. It is recommended to run cyte-seek on a cluster to reduce run times. At the current moment, the pipeline supports the following job schedulers: SLURM, UGE. slurm The slurm execution backend will submit jobs to the SLURM workload manager . This method will submit jobs to a cluster using sbatch. Please set the mode to slurm when running the pipeline on Biowulf. uge The uge execution backend will submit jobs to the UGE workload manager . This method will submit jobs to a cluster using qsub. Please set the mode to uge when running the pipeline on LOCUS. local Local executions will run serially on compute instance. This is useful for testing, debugging, or when a users does not have access to a high performance computing environment. If this option is not provided, it will default to a local execution mode. We do not recommend using this option. Example: --mode uge --job-name JOB_NAME Set the name of the pipeline's master job. type: string default: cyte-seek When submitting the pipeline to a job scheduler, like SLURM, this option always you to set the name of the pipeline's master job. By default, the name of the pipeline's master job is set to \"cyte-seek\". Example: --job-name pl_id-42 --singularity-cache SINGULARITY_CACHE Overrides the $SINGULARITY_CACHEDIR environment variable. type: path default: --output OUTPUT/.singularity Singularity will cache image layers pulled from remote registries. This ultimately speeds up the process of pull an image from DockerHub if an image layer already exists in the singularity cache directory. By default, the cache is set to the value provided to the --output argument. Please note that this cache cannot be shared across users. Singularity strictly enforces you own the cache directory and will return a non-zero exit code if you do not own the cache directory! See the --sif-cache option to create a shareable resource. Example: --singularity-cache /data/$USER/.singularity --sif-cache SIF_CACHE Path where a local cache of SIFs are stored. type: path Uses a local cache of SIFs on the filesystem. This SIF cache can be shared across users if permissions are set correctly. If a SIF does not exist in the SIF cache, the image will be pulled from Dockerhub and a warning message will be displayed. The cyte-seek cache subcommand can be used to create a local SIF cache. Please see cyte-seek cache for more information. This command is extremely useful for avoiding DockerHub pull rate limits. It also remove any potential errors that could occur due to network issues or DockerHub being temporarily unavailable. We recommend running cyte-seek with this option when ever possible. Example: --singularity-cache /data/$USER/SIFs --threads THREADS Max number of threads for local processes. type: int default: 2 Max number of threads for local processes. This option is more applicable when running the pipeline with --mode local . It is recommended setting this vaule to the maximum number of CPUs available on the host machine. Example: --threads 12 --tmp-dir TMP_DIR Max number of threads for each process. type: path default: /tmp/$USER Path on the file system for writing temporary output files. By default, the temporary directory is set to '/lscratch/$SLURM_JOBID' for backwards compatibility with the NIH's Biowulf cluster; however, if you are running the pipeline on another cluster, this option will need to be specified. Ideally, this path should point to a dedicated location on the filesystem for writing tmp files. On many systems, this location is set to somewhere in /scratch. If you need to inject a variable into this string that should NOT be expanded, please quote this options value in single quotes. Example: --tmp-dir /hpcdata/scratch/$USER","title":"2.4 Orchestration options"},{"location":"usage/run/#25-miscellaneous-options","text":"Each of the following arguments are optional, and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help","title":"2.5 Miscellaneous options"},{"location":"usage/run/#3-example","text":"","title":"3. Example"},{"location":"usage/run/#31-biowulf","text":"# Step 1.) Grab an interactive node, # do not run on head node! srun -N 1 -n 1 --time = 1 :00:00 --mem = 8gb --cpus-per-task = 2 --pty bash module purge module load singularity snakemake # Step 2A.) Dry-run the pipeline ./cyte-seek run --input .tests/*.R?.fastq.gz \\ --output /data/ $USER /cyte-seek_output \\ --features .tests/features.csv \\ --libraries libraries.csv \\ --genome hg38 \\ --mode slurm \\ --tmp-dir '/lscratch/$SLURM_JOBID' \\ --dry-run # Step 2B.) Run the cyte-seek pipeline # The slurm mode will submit jobs to # the cluster. It is recommended running # the pipeline in this mode on Biowulf. ./cyte-seek run --input .tests/*.R?.fastq.gz \\ --output /data/ $USER /cyte-seek_output \\ --features .tests/features.csv \\ --libraries libraries.csv \\ --genome hg38 \\ --mode slurm \\ --tmp-dir '/lscratch/$SLURM_JOBID'","title":"3.1 Biowulf"},{"location":"usage/run/#32-locus","text":"# Step 1.) Grab an interactive node, # do not run on head node! qrsh -l h_vmem = 4G -pe threaded 4 module load snakemake/6.0.5-Python-3.9.2 # Step 2A.) Dry-run the pipeline /hpcdata/rtb/NCBR/cyte-seek/v1.1.1/cyte-seek run \\ --input .tests/*.R?.fastq.gz \\ --output /hpcdata/scratch/ $USER /cyte-seek_output \\ --features .tests/features.csv \\ --libraries libraries.csv \\ --genome hg38 \\ --mode uge \\ --dry-run # Step 2B.) Run the cyte-seek pipeline # The slurm mode will submit jobs to # the cluster. It is recommended running # the pipeline in this mode on Biowulf. /hpcdata/rtb/NCBR/cyte-seek/v1.1.1/cyte-seek run \\ --input .tests/*.R?.fastq.gz \\ --output /hpcdata/scratch/ $USER /cyte-seek_output \\ --features .tests/features.csv \\ --libraries libraries.csv \\ --genome hg38 \\ --mode uge","title":"3.2 LOCUS"},{"location":"usage/unlock/","text":"cyte-seek unlock \u00b6 1. About \u00b6 The cyte-seek executable is composed of several inter-related sub commands. Please see cyte-seek -h for all available options. This part of the documentation describes options and concepts for cyte-seek unlock sub command in more detail. If the pipeline fails ungracefully, it maybe required to unlock the working directory before proceeding again. Snakemake will inform a user when it maybe necessary to unlock a working directory with an error message stating: Error: Directory cannot be locked . Please verify that the pipeline is not running before running this command. If the pipeline is currently running, the workflow manager will report the working directory is locked. The is the default behavior of snakemake, and it is normal. Do NOT run this command if the pipeline is still running! Please kill the master job and it's child jobs prior to running this command. Unlocking cyte-seek pipeline output directory is fast and easy! With minimal configuration, the unlock sub command enables you to unlock a pipeline output directory. In its most basic form, cyte-seek unlock only has one required input . 2. Synopsis \u00b6 $ ./cyte-seek unlock [-h] --output OUTPUT The synopsis for this command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide an output directory to unlock via --output argument. After running the unlock sub command, you can resume the build or run pipeline from where it left off by re-running it. Use you can always use the -h option for information on a specific command. 2.1 Required Arguments \u00b6 --output OUTPUT Output directory to unlock. type: path Path to a previous run's output directory. This will remove a lock on the working directory. Please verify that the pipeline is not running before running this command. Example: --output /data/$USER/output 2.2 Options \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help 3. Example \u00b6 # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load singularity snakemake # Step 1.) Unlock a pipeline output directory cyte-seek unlock --output /data/ $USER /output","title":"cyte-seek unlock"},{"location":"usage/unlock/#cyte-seek-unlock","text":"","title":"cyte-seek unlock"},{"location":"usage/unlock/#1-about","text":"The cyte-seek executable is composed of several inter-related sub commands. Please see cyte-seek -h for all available options. This part of the documentation describes options and concepts for cyte-seek unlock sub command in more detail. If the pipeline fails ungracefully, it maybe required to unlock the working directory before proceeding again. Snakemake will inform a user when it maybe necessary to unlock a working directory with an error message stating: Error: Directory cannot be locked . Please verify that the pipeline is not running before running this command. If the pipeline is currently running, the workflow manager will report the working directory is locked. The is the default behavior of snakemake, and it is normal. Do NOT run this command if the pipeline is still running! Please kill the master job and it's child jobs prior to running this command. Unlocking cyte-seek pipeline output directory is fast and easy! With minimal configuration, the unlock sub command enables you to unlock a pipeline output directory. In its most basic form, cyte-seek unlock only has one required input .","title":"1. About"},{"location":"usage/unlock/#2-synopsis","text":"$ ./cyte-seek unlock [-h] --output OUTPUT The synopsis for this command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide an output directory to unlock via --output argument. After running the unlock sub command, you can resume the build or run pipeline from where it left off by re-running it. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"usage/unlock/#21-required-arguments","text":"--output OUTPUT Output directory to unlock. type: path Path to a previous run's output directory. This will remove a lock on the working directory. Please verify that the pipeline is not running before running this command. Example: --output /data/$USER/output","title":"2.1 Required Arguments"},{"location":"usage/unlock/#22-options","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help","title":"2.2 Options"},{"location":"usage/unlock/#3-example","text":"# Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load singularity snakemake # Step 1.) Unlock a pipeline output directory cyte-seek unlock --output /data/ $USER /output","title":"3. Example"}]}